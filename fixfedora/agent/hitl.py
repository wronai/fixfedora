"""
Tryb Human-in-the-Loop (HITL) â€“ uÅ¼ytkownik zatwierdza kaÅ¼dÄ… akcjÄ™.
LLM proponuje, czÅ‚owiek decyduje, skrypt wykonuje.
"""

from __future__ import annotations

import signal
import subprocess
import time
from typing import Optional

try:
    from prompt_toolkit import PromptSession
    from prompt_toolkit.formatted_text import HTML
    from prompt_toolkit.styles import Style
    _HAS_PROMPT = True
except ImportError:
    _HAS_PROMPT = False

from ..providers.llm import LLMClient, LLMError
from ..utils.anonymizer import anonymize, display_anonymized_preview
from ..utils.web_search import search_all, format_results_for_llm
from ..config import FixFedoraConfig


SYSTEM_PROMPT = """JesteÅ› ekspertem diagnostyki Fedora Linux (specjalizacja: Lenovo Yoga, audio SOF/PipeWire, thumbnails).

Otrzymujesz anonimizowane dane diagnostyczne. Twoje zadania:

1. DIAGNOZA â€“ zidentyfikuj WSZYSTKIE problemy w danych (sorted by severity: ğŸ”´ krytyczne â†’ ğŸŸ¡ waÅ¼ne â†’ ğŸŸ¢ drobne)
2. ROZWIÄ„ZANIA â€“ dla kaÅ¼dego problemu podaj KONKRETNÄ„ komendÄ™ Fedora (dnf/systemctl/etc)
3. POTWIERDZENIE â€“ zawsze pytaj przed wykonaniem (tryb HITL)
4. WYJAÅšNIENIE â€“ krÃ³tko tÅ‚umacz co dana komenda robi i dlaczego

Format odpowiedzi diagnostycznej:
â”â”â” DIAGNOZA â”â”â”
ğŸ”´ Problem 1: [opis]
   â†’ Fix: `komenda`
ğŸŸ¡ Problem 2: [opis]
   â†’ Fix: `komenda`

Co robimy? (wpisz numer / 'all' / 'skip' / '!komenda' / 'search <zapytanie>' / 'q')

JeÅ›li pytasz o komendÄ™ do wykonania, uÅ¼yj DOKÅADNIE formatu:
EXEC: `<komenda>`
"""


class SessionTimeout(Exception):
    pass


def _timeout(signum, frame):
    raise SessionTimeout()


def _run_cmd_safely(cmd: str) -> tuple[bool, str]:
    """Wykonuje komendÄ™ z potwierdzeniem, dodaje sudo gdy potrzeba."""
    needs_sudo = any(
        cmd.strip().startswith(p)
        for p in ["dnf", "rpm", "systemctl", "firewall-cmd", "setenforce",
                  "chmod 0", "chown", "rm -rf", "mkfs", "fdisk", "parted"]
    )
    if needs_sudo and not cmd.strip().startswith("sudo"):
        cmd = "sudo " + cmd.strip()

    print(f"\n  â”Œâ”€ KOMENDA DO WYKONANIA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  â”‚  {cmd}")
    print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    confirm = input("  Potwierdzam (Y/n): ").strip().lower()
    if confirm not in ("y", "yes", ""):
        return False, "Anulowano."

    try:
        proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=120)
        out = proc.stdout.strip() or proc.stderr.strip() or "(brak outputu)"
        ok = proc.returncode == 0
        icon = "âœ…" if ok else f"âŒ (kod {proc.returncode})"
        print(f"  {icon}")
        if out:
            preview = out[:400] + ("..." if len(out) > 400 else "")
            print(f"  Output:\n{preview}")
        return ok, out
    except subprocess.TimeoutExpired:
        return False, "[TIMEOUT 120s]"
    except Exception as e:
        return False, f"[WYJÄ„TEK: {e}]"


def run_hitl_session(
    diagnostics: dict,
    config: FixFedoraConfig,
    show_data: bool = True,
):
    """
    Uruchamia interaktywnÄ… sesjÄ™ HITL.
    """
    llm = LLMClient(config)

    # Anonimizuj i opcjonalnie pokaÅ¼ dane
    anon_str, report = anonymize(str(diagnostics))
    if show_data:
        display_anonymized_preview(anon_str, report)
        print("\n  Czy wysÅ‚aÄ‡ te dane do LLM? [Y/n]: ", end="")
        ans = input().strip().lower()
        if ans in ("n", "no", "nie"):
            print("  Anulowano.")
            return

    # Konfiguracja timeout
    signal.signal(signal.SIGALRM, _timeout)
    signal.alarm(config.session_timeout)
    start_ts = time.time()

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                f"Oto dane diagnostyczne systemu Fedora (zanonimizowane):\n\n"
                f"```\n{anon_str}\n```\n\n"
                f"PrzeprowadÅº peÅ‚nÄ… analizÄ™ i wskaÅ¼ wszystkie wykryte problemy."
            ),
        },
    ]

    # Styl prompt
    style = Style.from_dict({"prompt": "#00cc00 bold", "info": "#888888"}) if _HAS_PROMPT else None
    session = PromptSession(style=style) if _HAS_PROMPT else None

    def remaining() -> int:
        return config.session_timeout - int(time.time() - start_ts)

    def fmt_time(s: int) -> str:
        return f"{s//3600:02d}:{(s%3600)//60:02d}:{s%60:02d}"

    print("\n" + "â•" * 65)
    print(f"  ğŸ‘¤ Tryb: HUMAN-IN-THE-LOOP  |  Model: {config.model}")
    print(f"  â° Sesja: max {fmt_time(config.session_timeout)}")
    print("  Komendy: [numer] fix | 'all' | 'skip' | '!cmd' | 'search <q>' | 'q'")
    print("â•" * 65 + "\n")

    web_search_count = 0
    MAX_WEB_SEARCHES = 3

    try:
        while True:
            rem = remaining()
            if rem <= 0:
                raise SessionTimeout()

            # WywoÅ‚anie LLM
            print("  ğŸ§  LLM analizuje...", end="", flush=True)
            try:
                reply = llm.chat(messages, max_tokens=2500, temperature=0.25)
                messages.append({"role": "assistant", "content": reply})
            except LLMError as e:
                print(f"\n  âŒ {e}")
                # Fallback: web search
                if config.enable_web_search and web_search_count < MAX_WEB_SEARCHES:
                    web_search_count += 1
                    print("  ğŸ” LLM niedostÄ™pny â€“ szukam w zewnÄ™trznych ÅºrÃ³dÅ‚ach...")
                    results = search_all("fedora system diagnostics", config.serpapi_key)
                    if results:
                        print(format_results_for_llm(results))
                break

            print("\r" + " " * 30 + "\r", end="")

            # SprawdÅº czy LLM sugeruje "nie wiem" â†’ web search
            low_conf_phrases = [
                "nie wiem", "nie jestem pewien", "brak informacji",
                "nie mam wystarczajÄ…cych", "skonsultuj siÄ™",
                "I don't know", "not sure", "cannot determine"
            ]
            llm_uncertain = any(p.lower() in reply.lower() for p in low_conf_phrases)

            print(f"\n{'â”€' * 65}")
            print(reply)
            print(f"{'â”€' * 65}")
            print(f"  â° {fmt_time(rem)}  |  Tokeny: ~{llm.total_tokens}")

            # JeÅ›li LLM niepewny i web search wÅ‚Ä…czony
            if llm_uncertain and config.enable_web_search and web_search_count < MAX_WEB_SEARCHES:
                print("\n  ğŸ’¡ LLM wskazaÅ‚ niepewnoÅ›Ä‡ â€“ szukam zewnÄ™trznie? [y/N]: ", end="")
                if input().strip().lower() in ("y", "yes", "tak"):
                    web_search_count += 1
                    # WyciÄ…gnij temat z ostatniej wiadomoÅ›ci
                    topic = _extract_search_topic(reply)
                    results = search_all(topic, config.serpapi_key)
                    if results:
                        web_context = format_results_for_llm(results)
                        print(web_context)
                        messages.append({
                            "role": "user",
                            "content": (
                                f"ZnalazÅ‚em nastÄ™pujÄ…ce zewnÄ™trzne ÅºrÃ³dÅ‚a â€“ "
                                f"uÅ¼yj ich do uzupeÅ‚nienia analizy:\n\n{web_context}"
                            ),
                        })
                        continue

            # Input uÅ¼ytkownika
            try:
                if session:
                    user_in = session.prompt(
                        HTML(f"\n<prompt>fixfedora</prompt> <info>[{fmt_time(rem)}]</info> â¯ ")
                    ).strip()
                else:
                    user_in = input(f"\nfixfedora [{fmt_time(rem)}] â¯ ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n  Sesja przerwana.")
                break

            if not user_in:
                continue

            # Komendy specjalne
            if user_in.lower() in ("q", "quit", "exit", "koniec"):
                print("\n  âœ… Sesja zakoÅ„czona.")
                break

            if user_in.startswith("!"):
                # BezpoÅ›rednie wykonanie komendy
                cmd = user_in[1:].strip()
                ok, out = _run_cmd_safely(cmd)
                messages.append({
                    "role": "user",
                    "content": f"WykonaÅ‚em: `{cmd}`\nWynik: {out}\nCo teraz?"
                })
                continue

            if user_in.lower().startswith("search "):
                # Manualne wyszukiwanie zewnÄ™trzne
                query = user_in[7:].strip()
                results = search_all(query, config.serpapi_key)
                if results:
                    web_context = format_results_for_llm(results)
                    print(web_context)
                    messages.append({
                        "role": "user",
                        "content": f"Wyniki wyszukiwania dla '{query}':\n{web_context}\nCo sÄ…dzisz?"
                    })
                else:
                    print("  Brak wynikÃ³w.")
                continue

            # SprawdÅº czy LLM zaproponowaÅ‚ EXEC: `komenda`
            import re
            exec_match = re.search(r"EXEC:\s*`([^`]+)`", reply)
            if user_in.isdigit() and exec_match:
                cmd = exec_match.group(1)
                ok, out = _run_cmd_safely(cmd)
                messages.append({
                    "role": "user",
                    "content": f"WykonaÅ‚em `{cmd}`. Wynik: {'sukces' if ok else 'bÅ‚Ä…d'}.\n{out}\nCo dalej?"
                })
                continue

            # Standardowy input do LLM
            messages.append({"role": "user", "content": user_in})

    except SessionTimeout:
        print(f"\n\n  â° Sesja wygasÅ‚a (limit: {fmt_time(config.session_timeout)}).")
    finally:
        signal.alarm(0)

    elapsed = int(time.time() - start_ts)
    print(f"\n  ğŸ“Š Sesja: {len(messages)-2} tur | {fmt_time(elapsed)} | ~{llm.total_tokens} tokenÃ³w")


def _extract_search_topic(llm_reply: str) -> str:
    """WyciÄ…ga sÅ‚owa kluczowe do wyszukiwania z odpowiedzi LLM."""
    # Szukaj konkretnych terminÃ³w technicznych
    import re
    tech_terms = re.findall(
        r"\b(sof-firmware|pipewire|alsa|thumbnails?|nautilus|"
        r"dnf|rpm|systemctl|journalctl|lenovo|yoga|codec|driver|"
        r"snd_hda|intel_sst|avs|wireplumber|pulseaudio)\b",
        llm_reply, re.IGNORECASE
    )
    if tech_terms:
        return " ".join(dict.fromkeys(tech_terms[:4]))
    # Fallback: pierwsze zdanie
    first_sentence = llm_reply.split(".")[0][:80]
    return first_sentence or "fedora diagnostics"
